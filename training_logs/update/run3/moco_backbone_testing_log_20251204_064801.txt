[2025-12-04 06:48:01] Starting MoCo backbone testing...
[2025-12-04 06:48:01] Loading pretrained MoCo model from /content/drive/MyDrive/Colab Notebooks/7_Py_DL/FP/artifacts/vit_hybrid_moco_encoder_v3.pth...
[2025-12-04 06:48:01] Loaded model. Missing keys: ['queue', 'queue_ptr', 'patch_embed.proj.0.weight', 'patch_embed.proj.0.bias', 'patch_embed.proj.2.weight', 'patch_embed.proj.2.bias', 'patch_embed.proj.4.weight', 'patch_embed.proj.4.bias', 'pos_encoding.pos_embed', 'pos_encoding.cls_token', 'transformer.encoder.layers.0.self_attn.in_proj_weight', 'transformer.encoder.layers.0.self_attn.in_proj_bias', 'transformer.encoder.layers.0.self_attn.out_proj.weight', 'transformer.encoder.layers.0.self_attn.out_proj.bias', 'transformer.encoder.layers.0.linear1.weight', 'transformer.encoder.layers.0.linear1.bias', 'transformer.encoder.layers.0.linear2.weight', 'transformer.encoder.layers.0.linear2.bias', 'transformer.encoder.layers.0.norm1.weight', 'transformer.encoder.layers.0.norm1.bias', 'transformer.encoder.layers.0.norm2.weight', 'transformer.encoder.layers.0.norm2.bias', 'transformer.encoder.layers.1.self_attn.in_proj_weight', 'transformer.encoder.layers.1.self_attn.in_proj_bias', 'transformer.encoder.layers.1.self_attn.out_proj.weight', 'transformer.encoder.layers.1.self_attn.out_proj.bias', 'transformer.encoder.layers.1.linear1.weight', 'transformer.encoder.layers.1.linear1.bias', 'transformer.encoder.layers.1.linear2.weight', 'transformer.encoder.layers.1.linear2.bias', 'transformer.encoder.layers.1.norm1.weight', 'transformer.encoder.layers.1.norm1.bias', 'transformer.encoder.layers.1.norm2.weight', 'transformer.encoder.layers.1.norm2.bias', 'transformer.encoder.layers.2.self_attn.in_proj_weight', 'transformer.encoder.layers.2.self_attn.in_proj_bias', 'transformer.encoder.layers.2.self_attn.out_proj.weight', 'transformer.encoder.layers.2.self_attn.out_proj.bias', 'transformer.encoder.layers.2.linear1.weight', 'transformer.encoder.layers.2.linear1.bias', 'transformer.encoder.layers.2.linear2.weight', 'transformer.encoder.layers.2.linear2.bias', 'transformer.encoder.layers.2.norm1.weight', 'transformer.encoder.layers.2.norm1.bias', 'transformer.encoder.layers.2.norm2.weight', 'transformer.encoder.layers.2.norm2.bias', 'transformer.encoder.layers.3.self_attn.in_proj_weight', 'transformer.encoder.layers.3.self_attn.in_proj_bias', 'transformer.encoder.layers.3.self_attn.out_proj.weight', 'transformer.encoder.layers.3.self_attn.out_proj.bias', 'transformer.encoder.layers.3.linear1.weight', 'transformer.encoder.layers.3.linear1.bias', 'transformer.encoder.layers.3.linear2.weight', 'transformer.encoder.layers.3.linear2.bias', 'transformer.encoder.layers.3.norm1.weight', 'transformer.encoder.layers.3.norm1.bias', 'transformer.encoder.layers.3.norm2.weight', 'transformer.encoder.layers.3.norm2.bias', 'transformer.encoder.layers.4.self_attn.in_proj_weight', 'transformer.encoder.layers.4.self_attn.in_proj_bias', 'transformer.encoder.layers.4.self_attn.out_proj.weight', 'transformer.encoder.layers.4.self_attn.out_proj.bias', 'transformer.encoder.layers.4.linear1.weight', 'transformer.encoder.layers.4.linear1.bias', 'transformer.encoder.layers.4.linear2.weight', 'transformer.encoder.layers.4.linear2.bias', 'transformer.encoder.layers.4.norm1.weight', 'transformer.encoder.layers.4.norm1.bias', 'transformer.encoder.layers.4.norm2.weight', 'transformer.encoder.layers.4.norm2.bias', 'transformer.encoder.layers.5.self_attn.in_proj_weight', 'transformer.encoder.layers.5.self_attn.in_proj_bias', 'transformer.encoder.layers.5.self_attn.out_proj.weight', 'transformer.encoder.layers.5.self_attn.out_proj.bias', 'transformer.encoder.layers.5.linear1.weight', 'transformer.encoder.layers.5.linear1.bias', 'transformer.encoder.layers.5.linear2.weight', 'transformer.encoder.layers.5.linear2.bias', 'transformer.encoder.layers.5.norm1.weight', 'transformer.encoder.layers.5.norm1.bias', 'transformer.encoder.layers.5.norm2.weight', 'transformer.encoder.layers.5.norm2.bias', 'transformer.encoder.layers.6.self_attn.in_proj_weight', 'transformer.encoder.layers.6.self_attn.in_proj_bias', 'transformer.encoder.layers.6.self_attn.out_proj.weight', 'transformer.encoder.layers.6.self_attn.out_proj.bias', 'transformer.encoder.layers.6.linear1.weight', 'transformer.encoder.layers.6.linear1.bias', 'transformer.encoder.layers.6.linear2.weight', 'transformer.encoder.layers.6.linear2.bias', 'transformer.encoder.layers.6.norm1.weight', 'transformer.encoder.layers.6.norm1.bias', 'transformer.encoder.layers.6.norm2.weight', 'transformer.encoder.layers.6.norm2.bias', 'transformer.encoder.layers.7.self_attn.in_proj_weight', 'transformer.encoder.layers.7.self_attn.in_proj_bias', 'transformer.encoder.layers.7.self_attn.out_proj.weight', 'transformer.encoder.layers.7.self_attn.out_proj.bias', 'transformer.encoder.layers.7.linear1.weight', 'transformer.encoder.layers.7.linear1.bias', 'transformer.encoder.layers.7.linear2.weight', 'transformer.encoder.layers.7.linear2.bias', 'transformer.encoder.layers.7.norm1.weight', 'transformer.encoder.layers.7.norm1.bias', 'transformer.encoder.layers.7.norm2.weight', 'transformer.encoder.layers.7.norm2.bias', 'transformer.encoder.layers.8.self_attn.in_proj_weight', 'transformer.encoder.layers.8.self_attn.in_proj_bias', 'transformer.encoder.layers.8.self_attn.out_proj.weight', 'transformer.encoder.layers.8.self_attn.out_proj.bias', 'transformer.encoder.layers.8.linear1.weight', 'transformer.encoder.layers.8.linear1.bias', 'transformer.encoder.layers.8.linear2.weight', 'transformer.encoder.layers.8.linear2.bias', 'transformer.encoder.layers.8.norm1.weight', 'transformer.encoder.layers.8.norm1.bias', 'transformer.encoder.layers.8.norm2.weight', 'transformer.encoder.layers.8.norm2.bias', 'transformer.encoder.layers.9.self_attn.in_proj_weight', 'transformer.encoder.layers.9.self_attn.in_proj_bias', 'transformer.encoder.layers.9.self_attn.out_proj.weight', 'transformer.encoder.layers.9.self_attn.out_proj.bias', 'transformer.encoder.layers.9.linear1.weight', 'transformer.encoder.layers.9.linear1.bias', 'transformer.encoder.layers.9.linear2.weight', 'transformer.encoder.layers.9.linear2.bias', 'transformer.encoder.layers.9.norm1.weight', 'transformer.encoder.layers.9.norm1.bias', 'transformer.encoder.layers.9.norm2.weight', 'transformer.encoder.layers.9.norm2.bias', 'transformer.encoder.layers.10.self_attn.in_proj_weight', 'transformer.encoder.layers.10.self_attn.in_proj_bias', 'transformer.encoder.layers.10.self_attn.out_proj.weight', 'transformer.encoder.layers.10.self_attn.out_proj.bias', 'transformer.encoder.layers.10.linear1.weight', 'transformer.encoder.layers.10.linear1.bias', 'transformer.encoder.layers.10.linear2.weight', 'transformer.encoder.layers.10.linear2.bias', 'transformer.encoder.layers.10.norm1.weight', 'transformer.encoder.layers.10.norm1.bias', 'transformer.encoder.layers.10.norm2.weight', 'transformer.encoder.layers.10.norm2.bias', 'transformer.encoder.layers.11.self_attn.in_proj_weight', 'transformer.encoder.layers.11.self_attn.in_proj_bias', 'transformer.encoder.layers.11.self_attn.out_proj.weight', 'transformer.encoder.layers.11.self_attn.out_proj.bias', 'transformer.encoder.layers.11.linear1.weight', 'transformer.encoder.layers.11.linear1.bias', 'transformer.encoder.layers.11.linear2.weight', 'transformer.encoder.layers.11.linear2.bias', 'transformer.encoder.layers.11.norm1.weight', 'transformer.encoder.layers.11.norm1.bias', 'transformer.encoder.layers.11.norm2.weight', 'transformer.encoder.layers.11.norm2.bias', 'proj_head.mlp.0.weight', 'proj_head.mlp.0.bias', 'proj_head.mlp.2.weight', 'proj_head.mlp.2.bias', 'patch_embed_k.proj.0.weight', 'patch_embed_k.proj.0.bias', 'patch_embed_k.proj.2.weight', 'patch_embed_k.proj.2.bias', 'patch_embed_k.proj.4.weight', 'patch_embed_k.proj.4.bias', 'pos_encoding_k.pos_embed', 'pos_encoding_k.cls_token', 'transformer_k.encoder.layers.0.self_attn.in_proj_weight', 'transformer_k.encoder.layers.0.self_attn.in_proj_bias', 'transformer_k.encoder.layers.0.self_attn.out_proj.weight', 'transformer_k.encoder.layers.0.self_attn.out_proj.bias', 'transformer_k.encoder.layers.0.linear1.weight', 'transformer_k.encoder.layers.0.linear1.bias', 'transformer_k.encoder.layers.0.linear2.weight', 'transformer_k.encoder.layers.0.linear2.bias', 'transformer_k.encoder.layers.0.norm1.weight', 'transformer_k.encoder.layers.0.norm1.bias', 'transformer_k.encoder.layers.0.norm2.weight', 'transformer_k.encoder.layers.0.norm2.bias', 'transformer_k.encoder.layers.1.self_attn.in_proj_weight', 'transformer_k.encoder.layers.1.self_attn.in_proj_bias', 'transformer_k.encoder.layers.1.self_attn.out_proj.weight', 'transformer_k.encoder.layers.1.self_attn.out_proj.bias', 'transformer_k.encoder.layers.1.linear1.weight', 'transformer_k.encoder.layers.1.linear1.bias', 'transformer_k.encoder.layers.1.linear2.weight', 'transformer_k.encoder.layers.1.linear2.bias', 'transformer_k.encoder.layers.1.norm1.weight', 'transformer_k.encoder.layers.1.norm1.bias', 'transformer_k.encoder.layers.1.norm2.weight', 'transformer_k.encoder.layers.1.norm2.bias', 'transformer_k.encoder.layers.2.self_attn.in_proj_weight', 'transformer_k.encoder.layers.2.self_attn.in_proj_bias', 'transformer_k.encoder.layers.2.self_attn.out_proj.weight', 'transformer_k.encoder.layers.2.self_attn.out_proj.bias', 'transformer_k.encoder.layers.2.linear1.weight', 'transformer_k.encoder.layers.2.linear1.bias', 'transformer_k.encoder.layers.2.linear2.weight', 'transformer_k.encoder.layers.2.linear2.bias', 'transformer_k.encoder.layers.2.norm1.weight', 'transformer_k.encoder.layers.2.norm1.bias', 'transformer_k.encoder.layers.2.norm2.weight', 'transformer_k.encoder.layers.2.norm2.bias', 'transformer_k.encoder.layers.3.self_attn.in_proj_weight', 'transformer_k.encoder.layers.3.self_attn.in_proj_bias', 'transformer_k.encoder.layers.3.self_attn.out_proj.weight', 'transformer_k.encoder.layers.3.self_attn.out_proj.bias', 'transformer_k.encoder.layers.3.linear1.weight', 'transformer_k.encoder.layers.3.linear1.bias', 'transformer_k.encoder.layers.3.linear2.weight', 'transformer_k.encoder.layers.3.linear2.bias', 'transformer_k.encoder.layers.3.norm1.weight', 'transformer_k.encoder.layers.3.norm1.bias', 'transformer_k.encoder.layers.3.norm2.weight', 'transformer_k.encoder.layers.3.norm2.bias', 'transformer_k.encoder.layers.4.self_attn.in_proj_weight', 'transformer_k.encoder.layers.4.self_attn.in_proj_bias', 'transformer_k.encoder.layers.4.self_attn.out_proj.weight', 'transformer_k.encoder.layers.4.self_attn.out_proj.bias', 'transformer_k.encoder.layers.4.linear1.weight', 'transformer_k.encoder.layers.4.linear1.bias', 'transformer_k.encoder.layers.4.linear2.weight', 'transformer_k.encoder.layers.4.linear2.bias', 'transformer_k.encoder.layers.4.norm1.weight', 'transformer_k.encoder.layers.4.norm1.bias', 'transformer_k.encoder.layers.4.norm2.weight', 'transformer_k.encoder.layers.4.norm2.bias', 'transformer_k.encoder.layers.5.self_attn.in_proj_weight', 'transformer_k.encoder.layers.5.self_attn.in_proj_bias', 'transformer_k.encoder.layers.5.self_attn.out_proj.weight', 'transformer_k.encoder.layers.5.self_attn.out_proj.bias', 'transformer_k.encoder.layers.5.linear1.weight', 'transformer_k.encoder.layers.5.linear1.bias', 'transformer_k.encoder.layers.5.linear2.weight', 'transformer_k.encoder.layers.5.linear2.bias', 'transformer_k.encoder.layers.5.norm1.weight', 'transformer_k.encoder.layers.5.norm1.bias', 'transformer_k.encoder.layers.5.norm2.weight', 'transformer_k.encoder.layers.5.norm2.bias', 'transformer_k.encoder.layers.6.self_attn.in_proj_weight', 'transformer_k.encoder.layers.6.self_attn.in_proj_bias', 'transformer_k.encoder.layers.6.self_attn.out_proj.weight', 'transformer_k.encoder.layers.6.self_attn.out_proj.bias', 'transformer_k.encoder.layers.6.linear1.weight', 'transformer_k.encoder.layers.6.linear1.bias', 'transformer_k.encoder.layers.6.linear2.weight', 'transformer_k.encoder.layers.6.linear2.bias', 'transformer_k.encoder.layers.6.norm1.weight', 'transformer_k.encoder.layers.6.norm1.bias', 'transformer_k.encoder.layers.6.norm2.weight', 'transformer_k.encoder.layers.6.norm2.bias', 'transformer_k.encoder.layers.7.self_attn.in_proj_weight', 'transformer_k.encoder.layers.7.self_attn.in_proj_bias', 'transformer_k.encoder.layers.7.self_attn.out_proj.weight', 'transformer_k.encoder.layers.7.self_attn.out_proj.bias', 'transformer_k.encoder.layers.7.linear1.weight', 'transformer_k.encoder.layers.7.linear1.bias', 'transformer_k.encoder.layers.7.linear2.weight', 'transformer_k.encoder.layers.7.linear2.bias', 'transformer_k.encoder.layers.7.norm1.weight', 'transformer_k.encoder.layers.7.norm1.bias', 'transformer_k.encoder.layers.7.norm2.weight', 'transformer_k.encoder.layers.7.norm2.bias', 'transformer_k.encoder.layers.8.self_attn.in_proj_weight', 'transformer_k.encoder.layers.8.self_attn.in_proj_bias', 'transformer_k.encoder.layers.8.self_attn.out_proj.weight', 'transformer_k.encoder.layers.8.self_attn.out_proj.bias', 'transformer_k.encoder.layers.8.linear1.weight', 'transformer_k.encoder.layers.8.linear1.bias', 'transformer_k.encoder.layers.8.linear2.weight', 'transformer_k.encoder.layers.8.linear2.bias', 'transformer_k.encoder.layers.8.norm1.weight', 'transformer_k.encoder.layers.8.norm1.bias', 'transformer_k.encoder.layers.8.norm2.weight', 'transformer_k.encoder.layers.8.norm2.bias', 'transformer_k.encoder.layers.9.self_attn.in_proj_weight', 'transformer_k.encoder.layers.9.self_attn.in_proj_bias', 'transformer_k.encoder.layers.9.self_attn.out_proj.weight', 'transformer_k.encoder.layers.9.self_attn.out_proj.bias', 'transformer_k.encoder.layers.9.linear1.weight', 'transformer_k.encoder.layers.9.linear1.bias', 'transformer_k.encoder.layers.9.linear2.weight', 'transformer_k.encoder.layers.9.linear2.bias', 'transformer_k.encoder.layers.9.norm1.weight', 'transformer_k.encoder.layers.9.norm1.bias', 'transformer_k.encoder.layers.9.norm2.weight', 'transformer_k.encoder.layers.9.norm2.bias', 'transformer_k.encoder.layers.10.self_attn.in_proj_weight', 'transformer_k.encoder.layers.10.self_attn.in_proj_bias', 'transformer_k.encoder.layers.10.self_attn.out_proj.weight', 'transformer_k.encoder.layers.10.self_attn.out_proj.bias', 'transformer_k.encoder.layers.10.linear1.weight', 'transformer_k.encoder.layers.10.linear1.bias', 'transformer_k.encoder.layers.10.linear2.weight', 'transformer_k.encoder.layers.10.linear2.bias', 'transformer_k.encoder.layers.10.norm1.weight', 'transformer_k.encoder.layers.10.norm1.bias', 'transformer_k.encoder.layers.10.norm2.weight', 'transformer_k.encoder.layers.10.norm2.bias', 'transformer_k.encoder.layers.11.self_attn.in_proj_weight', 'transformer_k.encoder.layers.11.self_attn.in_proj_bias', 'transformer_k.encoder.layers.11.self_attn.out_proj.weight', 'transformer_k.encoder.layers.11.self_attn.out_proj.bias', 'transformer_k.encoder.layers.11.linear1.weight', 'transformer_k.encoder.layers.11.linear1.bias', 'transformer_k.encoder.layers.11.linear2.weight', 'transformer_k.encoder.layers.11.linear2.bias', 'transformer_k.encoder.layers.11.norm1.weight', 'transformer_k.encoder.layers.11.norm1.bias', 'transformer_k.encoder.layers.11.norm2.weight', 'transformer_k.encoder.layers.11.norm2.bias', 'proj_head_k.mlp.0.weight', 'proj_head_k.mlp.0.bias', 'proj_head_k.mlp.2.weight', 'proj_head_k.mlp.2.bias'], unexpected keys: ['patch_embed', 'pos_encoding', 'transformer', 'proj_head', 'embedding_dim']
[2025-12-04 06:48:02] Loaded pretrained encoder. missing keys: ['queue', 'queue_ptr', 'patch_embed.proj.0.weight', 'patch_embed.proj.0.bias', 'patch_embed.proj.2.weight', 'patch_embed.proj.2.bias', 'patch_embed.proj.4.weight', 'patch_embed.proj.4.bias', 'pos_encoding.pos_embed', 'pos_encoding.cls_token', 'transformer.encoder.layers.0.self_attn.in_proj_weight', 'transformer.encoder.layers.0.self_attn.in_proj_bias', 'transformer.encoder.layers.0.self_attn.out_proj.weight', 'transformer.encoder.layers.0.self_attn.out_proj.bias', 'transformer.encoder.layers.0.linear1.weight', 'transformer.encoder.layers.0.linear1.bias', 'transformer.encoder.layers.0.linear2.weight', 'transformer.encoder.layers.0.linear2.bias', 'transformer.encoder.layers.0.norm1.weight', 'transformer.encoder.layers.0.norm1.bias', 'transformer.encoder.layers.0.norm2.weight', 'transformer.encoder.layers.0.norm2.bias', 'transformer.encoder.layers.1.self_attn.in_proj_weight', 'transformer.encoder.layers.1.self_attn.in_proj_bias', 'transformer.encoder.layers.1.self_attn.out_proj.weight', 'transformer.encoder.layers.1.self_attn.out_proj.bias', 'transformer.encoder.layers.1.linear1.weight', 'transformer.encoder.layers.1.linear1.bias', 'transformer.encoder.layers.1.linear2.weight', 'transformer.encoder.layers.1.linear2.bias', 'transformer.encoder.layers.1.norm1.weight', 'transformer.encoder.layers.1.norm1.bias', 'transformer.encoder.layers.1.norm2.weight', 'transformer.encoder.layers.1.norm2.bias', 'transformer.encoder.layers.2.self_attn.in_proj_weight', 'transformer.encoder.layers.2.self_attn.in_proj_bias', 'transformer.encoder.layers.2.self_attn.out_proj.weight', 'transformer.encoder.layers.2.self_attn.out_proj.bias', 'transformer.encoder.layers.2.linear1.weight', 'transformer.encoder.layers.2.linear1.bias', 'transformer.encoder.layers.2.linear2.weight', 'transformer.encoder.layers.2.linear2.bias', 'transformer.encoder.layers.2.norm1.weight', 'transformer.encoder.layers.2.norm1.bias', 'transformer.encoder.layers.2.norm2.weight', 'transformer.encoder.layers.2.norm2.bias', 'transformer.encoder.layers.3.self_attn.in_proj_weight', 'transformer.encoder.layers.3.self_attn.in_proj_bias', 'transformer.encoder.layers.3.self_attn.out_proj.weight', 'transformer.encoder.layers.3.self_attn.out_proj.bias', 'transformer.encoder.layers.3.linear1.weight', 'transformer.encoder.layers.3.linear1.bias', 'transformer.encoder.layers.3.linear2.weight', 'transformer.encoder.layers.3.linear2.bias', 'transformer.encoder.layers.3.norm1.weight', 'transformer.encoder.layers.3.norm1.bias', 'transformer.encoder.layers.3.norm2.weight', 'transformer.encoder.layers.3.norm2.bias', 'transformer.encoder.layers.4.self_attn.in_proj_weight', 'transformer.encoder.layers.4.self_attn.in_proj_bias', 'transformer.encoder.layers.4.self_attn.out_proj.weight', 'transformer.encoder.layers.4.self_attn.out_proj.bias', 'transformer.encoder.layers.4.linear1.weight', 'transformer.encoder.layers.4.linear1.bias', 'transformer.encoder.layers.4.linear2.weight', 'transformer.encoder.layers.4.linear2.bias', 'transformer.encoder.layers.4.norm1.weight', 'transformer.encoder.layers.4.norm1.bias', 'transformer.encoder.layers.4.norm2.weight', 'transformer.encoder.layers.4.norm2.bias', 'transformer.encoder.layers.5.self_attn.in_proj_weight', 'transformer.encoder.layers.5.self_attn.in_proj_bias', 'transformer.encoder.layers.5.self_attn.out_proj.weight', 'transformer.encoder.layers.5.self_attn.out_proj.bias', 'transformer.encoder.layers.5.linear1.weight', 'transformer.encoder.layers.5.linear1.bias', 'transformer.encoder.layers.5.linear2.weight', 'transformer.encoder.layers.5.linear2.bias', 'transformer.encoder.layers.5.norm1.weight', 'transformer.encoder.layers.5.norm1.bias', 'transformer.encoder.layers.5.norm2.weight', 'transformer.encoder.layers.5.norm2.bias', 'transformer.encoder.layers.6.self_attn.in_proj_weight', 'transformer.encoder.layers.6.self_attn.in_proj_bias', 'transformer.encoder.layers.6.self_attn.out_proj.weight', 'transformer.encoder.layers.6.self_attn.out_proj.bias', 'transformer.encoder.layers.6.linear1.weight', 'transformer.encoder.layers.6.linear1.bias', 'transformer.encoder.layers.6.linear2.weight', 'transformer.encoder.layers.6.linear2.bias', 'transformer.encoder.layers.6.norm1.weight', 'transformer.encoder.layers.6.norm1.bias', 'transformer.encoder.layers.6.norm2.weight', 'transformer.encoder.layers.6.norm2.bias', 'transformer.encoder.layers.7.self_attn.in_proj_weight', 'transformer.encoder.layers.7.self_attn.in_proj_bias', 'transformer.encoder.layers.7.self_attn.out_proj.weight', 'transformer.encoder.layers.7.self_attn.out_proj.bias', 'transformer.encoder.layers.7.linear1.weight', 'transformer.encoder.layers.7.linear1.bias', 'transformer.encoder.layers.7.linear2.weight', 'transformer.encoder.layers.7.linear2.bias', 'transformer.encoder.layers.7.norm1.weight', 'transformer.encoder.layers.7.norm1.bias', 'transformer.encoder.layers.7.norm2.weight', 'transformer.encoder.layers.7.norm2.bias', 'transformer.encoder.layers.8.self_attn.in_proj_weight', 'transformer.encoder.layers.8.self_attn.in_proj_bias', 'transformer.encoder.layers.8.self_attn.out_proj.weight', 'transformer.encoder.layers.8.self_attn.out_proj.bias', 'transformer.encoder.layers.8.linear1.weight', 'transformer.encoder.layers.8.linear1.bias', 'transformer.encoder.layers.8.linear2.weight', 'transformer.encoder.layers.8.linear2.bias', 'transformer.encoder.layers.8.norm1.weight', 'transformer.encoder.layers.8.norm1.bias', 'transformer.encoder.layers.8.norm2.weight', 'transformer.encoder.layers.8.norm2.bias', 'transformer.encoder.layers.9.self_attn.in_proj_weight', 'transformer.encoder.layers.9.self_attn.in_proj_bias', 'transformer.encoder.layers.9.self_attn.out_proj.weight', 'transformer.encoder.layers.9.self_attn.out_proj.bias', 'transformer.encoder.layers.9.linear1.weight', 'transformer.encoder.layers.9.linear1.bias', 'transformer.encoder.layers.9.linear2.weight', 'transformer.encoder.layers.9.linear2.bias', 'transformer.encoder.layers.9.norm1.weight', 'transformer.encoder.layers.9.norm1.bias', 'transformer.encoder.layers.9.norm2.weight', 'transformer.encoder.layers.9.norm2.bias', 'transformer.encoder.layers.10.self_attn.in_proj_weight', 'transformer.encoder.layers.10.self_attn.in_proj_bias', 'transformer.encoder.layers.10.self_attn.out_proj.weight', 'transformer.encoder.layers.10.self_attn.out_proj.bias', 'transformer.encoder.layers.10.linear1.weight', 'transformer.encoder.layers.10.linear1.bias', 'transformer.encoder.layers.10.linear2.weight', 'transformer.encoder.layers.10.linear2.bias', 'transformer.encoder.layers.10.norm1.weight', 'transformer.encoder.layers.10.norm1.bias', 'transformer.encoder.layers.10.norm2.weight', 'transformer.encoder.layers.10.norm2.bias', 'transformer.encoder.layers.11.self_attn.in_proj_weight', 'transformer.encoder.layers.11.self_attn.in_proj_bias', 'transformer.encoder.layers.11.self_attn.out_proj.weight', 'transformer.encoder.layers.11.self_attn.out_proj.bias', 'transformer.encoder.layers.11.linear1.weight', 'transformer.encoder.layers.11.linear1.bias', 'transformer.encoder.layers.11.linear2.weight', 'transformer.encoder.layers.11.linear2.bias', 'transformer.encoder.layers.11.norm1.weight', 'transformer.encoder.layers.11.norm1.bias', 'transformer.encoder.layers.11.norm2.weight', 'transformer.encoder.layers.11.norm2.bias', 'proj_head.mlp.0.weight', 'proj_head.mlp.0.bias', 'proj_head.mlp.2.weight', 'proj_head.mlp.2.bias', 'patch_embed_k.proj.0.weight', 'patch_embed_k.proj.0.bias', 'patch_embed_k.proj.2.weight', 'patch_embed_k.proj.2.bias', 'patch_embed_k.proj.4.weight', 'patch_embed_k.proj.4.bias', 'pos_encoding_k.pos_embed', 'pos_encoding_k.cls_token', 'transformer_k.encoder.layers.0.self_attn.in_proj_weight', 'transformer_k.encoder.layers.0.self_attn.in_proj_bias', 'transformer_k.encoder.layers.0.self_attn.out_proj.weight', 'transformer_k.encoder.layers.0.self_attn.out_proj.bias', 'transformer_k.encoder.layers.0.linear1.weight', 'transformer_k.encoder.layers.0.linear1.bias', 'transformer_k.encoder.layers.0.linear2.weight', 'transformer_k.encoder.layers.0.linear2.bias', 'transformer_k.encoder.layers.0.norm1.weight', 'transformer_k.encoder.layers.0.norm1.bias', 'transformer_k.encoder.layers.0.norm2.weight', 'transformer_k.encoder.layers.0.norm2.bias', 'transformer_k.encoder.layers.1.self_attn.in_proj_weight', 'transformer_k.encoder.layers.1.self_attn.in_proj_bias', 'transformer_k.encoder.layers.1.self_attn.out_proj.weight', 'transformer_k.encoder.layers.1.self_attn.out_proj.bias', 'transformer_k.encoder.layers.1.linear1.weight', 'transformer_k.encoder.layers.1.linear1.bias', 'transformer_k.encoder.layers.1.linear2.weight', 'transformer_k.encoder.layers.1.linear2.bias', 'transformer_k.encoder.layers.1.norm1.weight', 'transformer_k.encoder.layers.1.norm1.bias', 'transformer_k.encoder.layers.1.norm2.weight', 'transformer_k.encoder.layers.1.norm2.bias', 'transformer_k.encoder.layers.2.self_attn.in_proj_weight', 'transformer_k.encoder.layers.2.self_attn.in_proj_bias', 'transformer_k.encoder.layers.2.self_attn.out_proj.weight', 'transformer_k.encoder.layers.2.self_attn.out_proj.bias', 'transformer_k.encoder.layers.2.linear1.weight', 'transformer_k.encoder.layers.2.linear1.bias', 'transformer_k.encoder.layers.2.linear2.weight', 'transformer_k.encoder.layers.2.linear2.bias', 'transformer_k.encoder.layers.2.norm1.weight', 'transformer_k.encoder.layers.2.norm1.bias', 'transformer_k.encoder.layers.2.norm2.weight', 'transformer_k.encoder.layers.2.norm2.bias', 'transformer_k.encoder.layers.3.self_attn.in_proj_weight', 'transformer_k.encoder.layers.3.self_attn.in_proj_bias', 'transformer_k.encoder.layers.3.self_attn.out_proj.weight', 'transformer_k.encoder.layers.3.self_attn.out_proj.bias', 'transformer_k.encoder.layers.3.linear1.weight', 'transformer_k.encoder.layers.3.linear1.bias', 'transformer_k.encoder.layers.3.linear2.weight', 'transformer_k.encoder.layers.3.linear2.bias', 'transformer_k.encoder.layers.3.norm1.weight', 'transformer_k.encoder.layers.3.norm1.bias', 'transformer_k.encoder.layers.3.norm2.weight', 'transformer_k.encoder.layers.3.norm2.bias', 'transformer_k.encoder.layers.4.self_attn.in_proj_weight', 'transformer_k.encoder.layers.4.self_attn.in_proj_bias', 'transformer_k.encoder.layers.4.self_attn.out_proj.weight', 'transformer_k.encoder.layers.4.self_attn.out_proj.bias', 'transformer_k.encoder.layers.4.linear1.weight', 'transformer_k.encoder.layers.4.linear1.bias', 'transformer_k.encoder.layers.4.linear2.weight', 'transformer_k.encoder.layers.4.linear2.bias', 'transformer_k.encoder.layers.4.norm1.weight', 'transformer_k.encoder.layers.4.norm1.bias', 'transformer_k.encoder.layers.4.norm2.weight', 'transformer_k.encoder.layers.4.norm2.bias', 'transformer_k.encoder.layers.5.self_attn.in_proj_weight', 'transformer_k.encoder.layers.5.self_attn.in_proj_bias', 'transformer_k.encoder.layers.5.self_attn.out_proj.weight', 'transformer_k.encoder.layers.5.self_attn.out_proj.bias', 'transformer_k.encoder.layers.5.linear1.weight', 'transformer_k.encoder.layers.5.linear1.bias', 'transformer_k.encoder.layers.5.linear2.weight', 'transformer_k.encoder.layers.5.linear2.bias', 'transformer_k.encoder.layers.5.norm1.weight', 'transformer_k.encoder.layers.5.norm1.bias', 'transformer_k.encoder.layers.5.norm2.weight', 'transformer_k.encoder.layers.5.norm2.bias', 'transformer_k.encoder.layers.6.self_attn.in_proj_weight', 'transformer_k.encoder.layers.6.self_attn.in_proj_bias', 'transformer_k.encoder.layers.6.self_attn.out_proj.weight', 'transformer_k.encoder.layers.6.self_attn.out_proj.bias', 'transformer_k.encoder.layers.6.linear1.weight', 'transformer_k.encoder.layers.6.linear1.bias', 'transformer_k.encoder.layers.6.linear2.weight', 'transformer_k.encoder.layers.6.linear2.bias', 'transformer_k.encoder.layers.6.norm1.weight', 'transformer_k.encoder.layers.6.norm1.bias', 'transformer_k.encoder.layers.6.norm2.weight', 'transformer_k.encoder.layers.6.norm2.bias', 'transformer_k.encoder.layers.7.self_attn.in_proj_weight', 'transformer_k.encoder.layers.7.self_attn.in_proj_bias', 'transformer_k.encoder.layers.7.self_attn.out_proj.weight', 'transformer_k.encoder.layers.7.self_attn.out_proj.bias', 'transformer_k.encoder.layers.7.linear1.weight', 'transformer_k.encoder.layers.7.linear1.bias', 'transformer_k.encoder.layers.7.linear2.weight', 'transformer_k.encoder.layers.7.linear2.bias', 'transformer_k.encoder.layers.7.norm1.weight', 'transformer_k.encoder.layers.7.norm1.bias', 'transformer_k.encoder.layers.7.norm2.weight', 'transformer_k.encoder.layers.7.norm2.bias', 'transformer_k.encoder.layers.8.self_attn.in_proj_weight', 'transformer_k.encoder.layers.8.self_attn.in_proj_bias', 'transformer_k.encoder.layers.8.self_attn.out_proj.weight', 'transformer_k.encoder.layers.8.self_attn.out_proj.bias', 'transformer_k.encoder.layers.8.linear1.weight', 'transformer_k.encoder.layers.8.linear1.bias', 'transformer_k.encoder.layers.8.linear2.weight', 'transformer_k.encoder.layers.8.linear2.bias', 'transformer_k.encoder.layers.8.norm1.weight', 'transformer_k.encoder.layers.8.norm1.bias', 'transformer_k.encoder.layers.8.norm2.weight', 'transformer_k.encoder.layers.8.norm2.bias', 'transformer_k.encoder.layers.9.self_attn.in_proj_weight', 'transformer_k.encoder.layers.9.self_attn.in_proj_bias', 'transformer_k.encoder.layers.9.self_attn.out_proj.weight', 'transformer_k.encoder.layers.9.self_attn.out_proj.bias', 'transformer_k.encoder.layers.9.linear1.weight', 'transformer_k.encoder.layers.9.linear1.bias', 'transformer_k.encoder.layers.9.linear2.weight', 'transformer_k.encoder.layers.9.linear2.bias', 'transformer_k.encoder.layers.9.norm1.weight', 'transformer_k.encoder.layers.9.norm1.bias', 'transformer_k.encoder.layers.9.norm2.weight', 'transformer_k.encoder.layers.9.norm2.bias', 'transformer_k.encoder.layers.10.self_attn.in_proj_weight', 'transformer_k.encoder.layers.10.self_attn.in_proj_bias', 'transformer_k.encoder.layers.10.self_attn.out_proj.weight', 'transformer_k.encoder.layers.10.self_attn.out_proj.bias', 'transformer_k.encoder.layers.10.linear1.weight', 'transformer_k.encoder.layers.10.linear1.bias', 'transformer_k.encoder.layers.10.linear2.weight', 'transformer_k.encoder.layers.10.linear2.bias', 'transformer_k.encoder.layers.10.norm1.weight', 'transformer_k.encoder.layers.10.norm1.bias', 'transformer_k.encoder.layers.10.norm2.weight', 'transformer_k.encoder.layers.10.norm2.bias', 'transformer_k.encoder.layers.11.self_attn.in_proj_weight', 'transformer_k.encoder.layers.11.self_attn.in_proj_bias', 'transformer_k.encoder.layers.11.self_attn.out_proj.weight', 'transformer_k.encoder.layers.11.self_attn.out_proj.bias', 'transformer_k.encoder.layers.11.linear1.weight', 'transformer_k.encoder.layers.11.linear1.bias', 'transformer_k.encoder.layers.11.linear2.weight', 'transformer_k.encoder.layers.11.linear2.bias', 'transformer_k.encoder.layers.11.norm1.weight', 'transformer_k.encoder.layers.11.norm1.bias', 'transformer_k.encoder.layers.11.norm2.weight', 'transformer_k.encoder.layers.11.norm2.bias', 'proj_head_k.mlp.0.weight', 'proj_head_k.mlp.0.bias', 'proj_head_k.mlp.2.weight', 'proj_head_k.mlp.2.bias'], unexpected: ['patch_embed', 'pos_encoding', 'transformer', 'proj_head', 'embedding_dim']
[2025-12-04 06:48:03] Starting MoCo backbone testing...
[2025-12-04 06:48:03] Model Keys: odict_keys(['queue', 'queue_ptr', 'patch_embed.proj.0.weight', 'patch_embed.proj.0.bias', 'patch_embed.proj.2.weight', 'patch_embed.proj.2.bias', 'patch_embed.proj.4.weight', 'patch_embed.proj.4.bias', 'pos_encoding.pos_embed', 'pos_encoding.cls_token', 'transformer.encoder.layers.0.self_attn.in_proj_weight', 'transformer.encoder.layers.0.self_attn.in_proj_bias', 'transformer.encoder.layers.0.self_attn.out_proj.weight', 'transformer.encoder.layers.0.self_attn.out_proj.bias', 'transformer.encoder.layers.0.linear1.weight', 'transformer.encoder.layers.0.linear1.bias', 'transformer.encoder.layers.0.linear2.weight', 'transformer.encoder.layers.0.linear2.bias', 'transformer.encoder.layers.0.norm1.weight', 'transformer.encoder.layers.0.norm1.bias', 'transformer.encoder.layers.0.norm2.weight', 'transformer.encoder.layers.0.norm2.bias', 'transformer.encoder.layers.1.self_attn.in_proj_weight', 'transformer.encoder.layers.1.self_attn.in_proj_bias', 'transformer.encoder.layers.1.self_attn.out_proj.weight', 'transformer.encoder.layers.1.self_attn.out_proj.bias', 'transformer.encoder.layers.1.linear1.weight', 'transformer.encoder.layers.1.linear1.bias', 'transformer.encoder.layers.1.linear2.weight', 'transformer.encoder.layers.1.linear2.bias', 'transformer.encoder.layers.1.norm1.weight', 'transformer.encoder.layers.1.norm1.bias', 'transformer.encoder.layers.1.norm2.weight', 'transformer.encoder.layers.1.norm2.bias', 'transformer.encoder.layers.2.self_attn.in_proj_weight', 'transformer.encoder.layers.2.self_attn.in_proj_bias', 'transformer.encoder.layers.2.self_attn.out_proj.weight', 'transformer.encoder.layers.2.self_attn.out_proj.bias', 'transformer.encoder.layers.2.linear1.weight', 'transformer.encoder.layers.2.linear1.bias', 'transformer.encoder.layers.2.linear2.weight', 'transformer.encoder.layers.2.linear2.bias', 'transformer.encoder.layers.2.norm1.weight', 'transformer.encoder.layers.2.norm1.bias', 'transformer.encoder.layers.2.norm2.weight', 'transformer.encoder.layers.2.norm2.bias', 'transformer.encoder.layers.3.self_attn.in_proj_weight', 'transformer.encoder.layers.3.self_attn.in_proj_bias', 'transformer.encoder.layers.3.self_attn.out_proj.weight', 'transformer.encoder.layers.3.self_attn.out_proj.bias', 'transformer.encoder.layers.3.linear1.weight', 'transformer.encoder.layers.3.linear1.bias', 'transformer.encoder.layers.3.linear2.weight', 'transformer.encoder.layers.3.linear2.bias', 'transformer.encoder.layers.3.norm1.weight', 'transformer.encoder.layers.3.norm1.bias', 'transformer.encoder.layers.3.norm2.weight', 'transformer.encoder.layers.3.norm2.bias', 'transformer.encoder.layers.4.self_attn.in_proj_weight', 'transformer.encoder.layers.4.self_attn.in_proj_bias', 'transformer.encoder.layers.4.self_attn.out_proj.weight', 'transformer.encoder.layers.4.self_attn.out_proj.bias', 'transformer.encoder.layers.4.linear1.weight', 'transformer.encoder.layers.4.linear1.bias', 'transformer.encoder.layers.4.linear2.weight', 'transformer.encoder.layers.4.linear2.bias', 'transformer.encoder.layers.4.norm1.weight', 'transformer.encoder.layers.4.norm1.bias', 'transformer.encoder.layers.4.norm2.weight', 'transformer.encoder.layers.4.norm2.bias', 'transformer.encoder.layers.5.self_attn.in_proj_weight', 'transformer.encoder.layers.5.self_attn.in_proj_bias', 'transformer.encoder.layers.5.self_attn.out_proj.weight', 'transformer.encoder.layers.5.self_attn.out_proj.bias', 'transformer.encoder.layers.5.linear1.weight', 'transformer.encoder.layers.5.linear1.bias', 'transformer.encoder.layers.5.linear2.weight', 'transformer.encoder.layers.5.linear2.bias', 'transformer.encoder.layers.5.norm1.weight', 'transformer.encoder.layers.5.norm1.bias', 'transformer.encoder.layers.5.norm2.weight', 'transformer.encoder.layers.5.norm2.bias', 'transformer.encoder.layers.6.self_attn.in_proj_weight', 'transformer.encoder.layers.6.self_attn.in_proj_bias', 'transformer.encoder.layers.6.self_attn.out_proj.weight', 'transformer.encoder.layers.6.self_attn.out_proj.bias', 'transformer.encoder.layers.6.linear1.weight', 'transformer.encoder.layers.6.linear1.bias', 'transformer.encoder.layers.6.linear2.weight', 'transformer.encoder.layers.6.linear2.bias', 'transformer.encoder.layers.6.norm1.weight', 'transformer.encoder.layers.6.norm1.bias', 'transformer.encoder.layers.6.norm2.weight', 'transformer.encoder.layers.6.norm2.bias', 'transformer.encoder.layers.7.self_attn.in_proj_weight', 'transformer.encoder.layers.7.self_attn.in_proj_bias', 'transformer.encoder.layers.7.self_attn.out_proj.weight', 'transformer.encoder.layers.7.self_attn.out_proj.bias', 'transformer.encoder.layers.7.linear1.weight', 'transformer.encoder.layers.7.linear1.bias', 'transformer.encoder.layers.7.linear2.weight', 'transformer.encoder.layers.7.linear2.bias', 'transformer.encoder.layers.7.norm1.weight', 'transformer.encoder.layers.7.norm1.bias', 'transformer.encoder.layers.7.norm2.weight', 'transformer.encoder.layers.7.norm2.bias', 'transformer.encoder.layers.8.self_attn.in_proj_weight', 'transformer.encoder.layers.8.self_attn.in_proj_bias', 'transformer.encoder.layers.8.self_attn.out_proj.weight', 'transformer.encoder.layers.8.self_attn.out_proj.bias', 'transformer.encoder.layers.8.linear1.weight', 'transformer.encoder.layers.8.linear1.bias', 'transformer.encoder.layers.8.linear2.weight', 'transformer.encoder.layers.8.linear2.bias', 'transformer.encoder.layers.8.norm1.weight', 'transformer.encoder.layers.8.norm1.bias', 'transformer.encoder.layers.8.norm2.weight', 'transformer.encoder.layers.8.norm2.bias', 'transformer.encoder.layers.9.self_attn.in_proj_weight', 'transformer.encoder.layers.9.self_attn.in_proj_bias', 'transformer.encoder.layers.9.self_attn.out_proj.weight', 'transformer.encoder.layers.9.self_attn.out_proj.bias', 'transformer.encoder.layers.9.linear1.weight', 'transformer.encoder.layers.9.linear1.bias', 'transformer.encoder.layers.9.linear2.weight', 'transformer.encoder.layers.9.linear2.bias', 'transformer.encoder.layers.9.norm1.weight', 'transformer.encoder.layers.9.norm1.bias', 'transformer.encoder.layers.9.norm2.weight', 'transformer.encoder.layers.9.norm2.bias', 'transformer.encoder.layers.10.self_attn.in_proj_weight', 'transformer.encoder.layers.10.self_attn.in_proj_bias', 'transformer.encoder.layers.10.self_attn.out_proj.weight', 'transformer.encoder.layers.10.self_attn.out_proj.bias', 'transformer.encoder.layers.10.linear1.weight', 'transformer.encoder.layers.10.linear1.bias', 'transformer.encoder.layers.10.linear2.weight', 'transformer.encoder.layers.10.linear2.bias', 'transformer.encoder.layers.10.norm1.weight', 'transformer.encoder.layers.10.norm1.bias', 'transformer.encoder.layers.10.norm2.weight', 'transformer.encoder.layers.10.norm2.bias', 'transformer.encoder.layers.11.self_attn.in_proj_weight', 'transformer.encoder.layers.11.self_attn.in_proj_bias', 'transformer.encoder.layers.11.self_attn.out_proj.weight', 'transformer.encoder.layers.11.self_attn.out_proj.bias', 'transformer.encoder.layers.11.linear1.weight', 'transformer.encoder.layers.11.linear1.bias', 'transformer.encoder.layers.11.linear2.weight', 'transformer.encoder.layers.11.linear2.bias', 'transformer.encoder.layers.11.norm1.weight', 'transformer.encoder.layers.11.norm1.bias', 'transformer.encoder.layers.11.norm2.weight', 'transformer.encoder.layers.11.norm2.bias', 'proj_head.mlp.0.weight', 'proj_head.mlp.0.bias', 'proj_head.mlp.2.weight', 'proj_head.mlp.2.bias', 'patch_embed_k.proj.0.weight', 'patch_embed_k.proj.0.bias', 'patch_embed_k.proj.2.weight', 'patch_embed_k.proj.2.bias', 'patch_embed_k.proj.4.weight', 'patch_embed_k.proj.4.bias', 'pos_encoding_k.pos_embed', 'pos_encoding_k.cls_token', 'transformer_k.encoder.layers.0.self_attn.in_proj_weight', 'transformer_k.encoder.layers.0.self_attn.in_proj_bias', 'transformer_k.encoder.layers.0.self_attn.out_proj.weight', 'transformer_k.encoder.layers.0.self_attn.out_proj.bias', 'transformer_k.encoder.layers.0.linear1.weight', 'transformer_k.encoder.layers.0.linear1.bias', 'transformer_k.encoder.layers.0.linear2.weight', 'transformer_k.encoder.layers.0.linear2.bias', 'transformer_k.encoder.layers.0.norm1.weight', 'transformer_k.encoder.layers.0.norm1.bias', 'transformer_k.encoder.layers.0.norm2.weight', 'transformer_k.encoder.layers.0.norm2.bias', 'transformer_k.encoder.layers.1.self_attn.in_proj_weight', 'transformer_k.encoder.layers.1.self_attn.in_proj_bias', 'transformer_k.encoder.layers.1.self_attn.out_proj.weight', 'transformer_k.encoder.layers.1.self_attn.out_proj.bias', 'transformer_k.encoder.layers.1.linear1.weight', 'transformer_k.encoder.layers.1.linear1.bias', 'transformer_k.encoder.layers.1.linear2.weight', 'transformer_k.encoder.layers.1.linear2.bias', 'transformer_k.encoder.layers.1.norm1.weight', 'transformer_k.encoder.layers.1.norm1.bias', 'transformer_k.encoder.layers.1.norm2.weight', 'transformer_k.encoder.layers.1.norm2.bias', 'transformer_k.encoder.layers.2.self_attn.in_proj_weight', 'transformer_k.encoder.layers.2.self_attn.in_proj_bias', 'transformer_k.encoder.layers.2.self_attn.out_proj.weight', 'transformer_k.encoder.layers.2.self_attn.out_proj.bias', 'transformer_k.encoder.layers.2.linear1.weight', 'transformer_k.encoder.layers.2.linear1.bias', 'transformer_k.encoder.layers.2.linear2.weight', 'transformer_k.encoder.layers.2.linear2.bias', 'transformer_k.encoder.layers.2.norm1.weight', 'transformer_k.encoder.layers.2.norm1.bias', 'transformer_k.encoder.layers.2.norm2.weight', 'transformer_k.encoder.layers.2.norm2.bias', 'transformer_k.encoder.layers.3.self_attn.in_proj_weight', 'transformer_k.encoder.layers.3.self_attn.in_proj_bias', 'transformer_k.encoder.layers.3.self_attn.out_proj.weight', 'transformer_k.encoder.layers.3.self_attn.out_proj.bias', 'transformer_k.encoder.layers.3.linear1.weight', 'transformer_k.encoder.layers.3.linear1.bias', 'transformer_k.encoder.layers.3.linear2.weight', 'transformer_k.encoder.layers.3.linear2.bias', 'transformer_k.encoder.layers.3.norm1.weight', 'transformer_k.encoder.layers.3.norm1.bias', 'transformer_k.encoder.layers.3.norm2.weight', 'transformer_k.encoder.layers.3.norm2.bias', 'transformer_k.encoder.layers.4.self_attn.in_proj_weight', 'transformer_k.encoder.layers.4.self_attn.in_proj_bias', 'transformer_k.encoder.layers.4.self_attn.out_proj.weight', 'transformer_k.encoder.layers.4.self_attn.out_proj.bias', 'transformer_k.encoder.layers.4.linear1.weight', 'transformer_k.encoder.layers.4.linear1.bias', 'transformer_k.encoder.layers.4.linear2.weight', 'transformer_k.encoder.layers.4.linear2.bias', 'transformer_k.encoder.layers.4.norm1.weight', 'transformer_k.encoder.layers.4.norm1.bias', 'transformer_k.encoder.layers.4.norm2.weight', 'transformer_k.encoder.layers.4.norm2.bias', 'transformer_k.encoder.layers.5.self_attn.in_proj_weight', 'transformer_k.encoder.layers.5.self_attn.in_proj_bias', 'transformer_k.encoder.layers.5.self_attn.out_proj.weight', 'transformer_k.encoder.layers.5.self_attn.out_proj.bias', 'transformer_k.encoder.layers.5.linear1.weight', 'transformer_k.encoder.layers.5.linear1.bias', 'transformer_k.encoder.layers.5.linear2.weight', 'transformer_k.encoder.layers.5.linear2.bias', 'transformer_k.encoder.layers.5.norm1.weight', 'transformer_k.encoder.layers.5.norm1.bias', 'transformer_k.encoder.layers.5.norm2.weight', 'transformer_k.encoder.layers.5.norm2.bias', 'transformer_k.encoder.layers.6.self_attn.in_proj_weight', 'transformer_k.encoder.layers.6.self_attn.in_proj_bias', 'transformer_k.encoder.layers.6.self_attn.out_proj.weight', 'transformer_k.encoder.layers.6.self_attn.out_proj.bias', 'transformer_k.encoder.layers.6.linear1.weight', 'transformer_k.encoder.layers.6.linear1.bias', 'transformer_k.encoder.layers.6.linear2.weight', 'transformer_k.encoder.layers.6.linear2.bias', 'transformer_k.encoder.layers.6.norm1.weight', 'transformer_k.encoder.layers.6.norm1.bias', 'transformer_k.encoder.layers.6.norm2.weight', 'transformer_k.encoder.layers.6.norm2.bias', 'transformer_k.encoder.layers.7.self_attn.in_proj_weight', 'transformer_k.encoder.layers.7.self_attn.in_proj_bias', 'transformer_k.encoder.layers.7.self_attn.out_proj.weight', 'transformer_k.encoder.layers.7.self_attn.out_proj.bias', 'transformer_k.encoder.layers.7.linear1.weight', 'transformer_k.encoder.layers.7.linear1.bias', 'transformer_k.encoder.layers.7.linear2.weight', 'transformer_k.encoder.layers.7.linear2.bias', 'transformer_k.encoder.layers.7.norm1.weight', 'transformer_k.encoder.layers.7.norm1.bias', 'transformer_k.encoder.layers.7.norm2.weight', 'transformer_k.encoder.layers.7.norm2.bias', 'transformer_k.encoder.layers.8.self_attn.in_proj_weight', 'transformer_k.encoder.layers.8.self_attn.in_proj_bias', 'transformer_k.encoder.layers.8.self_attn.out_proj.weight', 'transformer_k.encoder.layers.8.self_attn.out_proj.bias', 'transformer_k.encoder.layers.8.linear1.weight', 'transformer_k.encoder.layers.8.linear1.bias', 'transformer_k.encoder.layers.8.linear2.weight', 'transformer_k.encoder.layers.8.linear2.bias', 'transformer_k.encoder.layers.8.norm1.weight', 'transformer_k.encoder.layers.8.norm1.bias', 'transformer_k.encoder.layers.8.norm2.weight', 'transformer_k.encoder.layers.8.norm2.bias', 'transformer_k.encoder.layers.9.self_attn.in_proj_weight', 'transformer_k.encoder.layers.9.self_attn.in_proj_bias', 'transformer_k.encoder.layers.9.self_attn.out_proj.weight', 'transformer_k.encoder.layers.9.self_attn.out_proj.bias', 'transformer_k.encoder.layers.9.linear1.weight', 'transformer_k.encoder.layers.9.linear1.bias', 'transformer_k.encoder.layers.9.linear2.weight', 'transformer_k.encoder.layers.9.linear2.bias', 'transformer_k.encoder.layers.9.norm1.weight', 'transformer_k.encoder.layers.9.norm1.bias', 'transformer_k.encoder.layers.9.norm2.weight', 'transformer_k.encoder.layers.9.norm2.bias', 'transformer_k.encoder.layers.10.self_attn.in_proj_weight', 'transformer_k.encoder.layers.10.self_attn.in_proj_bias', 'transformer_k.encoder.layers.10.self_attn.out_proj.weight', 'transformer_k.encoder.layers.10.self_attn.out_proj.bias', 'transformer_k.encoder.layers.10.linear1.weight', 'transformer_k.encoder.layers.10.linear1.bias', 'transformer_k.encoder.layers.10.linear2.weight', 'transformer_k.encoder.layers.10.linear2.bias', 'transformer_k.encoder.layers.10.norm1.weight', 'transformer_k.encoder.layers.10.norm1.bias', 'transformer_k.encoder.layers.10.norm2.weight', 'transformer_k.encoder.layers.10.norm2.bias', 'transformer_k.encoder.layers.11.self_attn.in_proj_weight', 'transformer_k.encoder.layers.11.self_attn.in_proj_bias', 'transformer_k.encoder.layers.11.self_attn.out_proj.weight', 'transformer_k.encoder.layers.11.self_attn.out_proj.bias', 'transformer_k.encoder.layers.11.linear1.weight', 'transformer_k.encoder.layers.11.linear1.bias', 'transformer_k.encoder.layers.11.linear2.weight', 'transformer_k.encoder.layers.11.linear2.bias', 'transformer_k.encoder.layers.11.norm1.weight', 'transformer_k.encoder.layers.11.norm1.bias', 'transformer_k.encoder.layers.11.norm2.weight', 'transformer_k.encoder.layers.11.norm2.bias', 'proj_head_k.mlp.0.weight', 'proj_head_k.mlp.0.bias', 'proj_head_k.mlp.2.weight', 'proj_head_k.mlp.2.bias'])
[2025-12-04 06:48:03] Starting linear evaluation for 10 epochs...
[2025-12-04 06:50:24] Epoch 1/10, Avg. Running Loss: 7.609862', Train Acc:32.70% Test Acc: 17.77%
[2025-12-04 06:52:44] Epoch 2/10, Avg. Running Loss: 8.491882', Train Acc:32.71% Test Acc: 17.77%
[2025-12-04 06:55:05] Epoch 3/10, Avg. Running Loss: 6.113725', Train Acc:67.30% Test Acc: 82.23%
[2025-12-04 06:57:26] Epoch 4/10, Avg. Running Loss: 10.216507', Train Acc:39.42% Test Acc: 28.84%
[2025-12-04 06:59:47] Epoch 5/10, Avg. Running Loss: 13.801026', Train Acc:67.30% Test Acc: 82.23%
[2025-12-04 07:02:08] Epoch 6/10, Avg. Running Loss: 5.820257', Train Acc:67.30% Test Acc: 82.23%
[2025-12-04 07:04:29] Epoch 7/10, Avg. Running Loss: 8.466195', Train Acc:66.31% Test Acc: 80.06%
[2025-12-04 07:06:50] Epoch 8/10, Avg. Running Loss: 9.924735', Train Acc:63.98% Test Acc: 75.54%
[2025-12-04 07:09:11] Epoch 9/10, Avg. Running Loss: 6.597820', Train Acc:67.30% Test Acc: 82.23%
[2025-12-04 07:11:31] Epoch 10/10, Avg. Running Loss: 6.203107', Train Acc:67.30% Test Acc: 82.23%
[2025-12-04 07:11:45] Classification Report:
              precision    recall  f1-score   support

           0       0.82      1.00      0.90      2800
           1       0.00      0.00      0.00       605

    accuracy                           0.82      3405
   macro avg       0.41      0.50      0.45      3405
weighted avg       0.68      0.82      0.74      3405

[2025-12-04 07:11:45] Confusion Matrix:
[[2800    0]
 [ 605    0]]
[2025-12-04 07:11:45] Saved confusion matrix to /content/drive/MyDrive/Colab Notebooks/7_Py_DL/FP/artifacts/confusion_matrix_20251204_071131.png
[2025-12-04 07:11:45] MoCo backbone testing complete!
[2025-12-04 07:11:45] MoCo backbone testing complete!!
[2025-12-04 07:11:45] Saved testing stats to /content/drive/MyDrive/Colab Notebooks/7_Py_DL/FP/artifacts//test_stats.json
